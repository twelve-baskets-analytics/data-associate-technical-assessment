{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: transactions.transaction_id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     17\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     18\u001b[0m transaction_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     (i, \n\u001b[1;32m     20\u001b[0m      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-08-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m31\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m)\n\u001b[1;32m     26\u001b[0m ]\n\u001b[0;32m---> 28\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43mINSERT INTO transactions (transaction_id, transaction_date, product_id, quantity, price)\u001b[39;49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;43mVALUES (?, ?, ?, ?, ?)\u001b[39;49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransaction_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLite Transactional table created and populated with sample data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIntegrityError\u001b[0m: UNIQUE constraint failed: transactions.transaction_id"
     ]
    }
   ],
   "source": [
    "# Create a connection to SQLite database (or create a new one)\n",
    "conn = sqlite3.connect('transactional_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a transactional table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS transactions (\n",
    "    transaction_id INTEGER PRIMARY KEY,\n",
    "    transaction_date TEXT,\n",
    "    product_id INTEGER,\n",
    "    quantity INTEGER,\n",
    "    price REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert some sample data into the transactional table\n",
    "np.random.seed(42)\n",
    "transaction_data = [\n",
    "    (i, \n",
    "     f\"2024-08-{np.random.randint(1, 31):02d}\", \n",
    "     np.random.randint(1, 11), \n",
    "     np.random.randint(1, 10), \n",
    "     round(np.random.uniform(5.0, 100.0), 2)\n",
    "    )\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "cursor.executemany('''\n",
    "INSERT INTO transactions (transaction_id, transaction_date, product_id, quantity, price)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "''', transaction_data)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(\"SQLite Transactional table created and populated with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with product categories created.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with product_id and product_category\n",
    "product_category_data = {\n",
    "    'product_id': range(1, 11),\n",
    "    'product_category': ['Electronics', 'Clothing', 'Groceries', 'Furniture', 'Toys', \n",
    "                         'Books', 'Sports', 'Beauty', 'Automotive', 'Jewelry']\n",
    "}\n",
    "\n",
    "product_category_df = pd.DataFrame(product_category_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "product_category_df.to_csv('product_categories.csv', index=False)\n",
    "\n",
    "print(\"CSV file with product categories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created and saved as 'synthetic_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a synthetic dataset with 10,000 rows and 50 features\n",
    "X, y = make_classification(n_samples=10000, \n",
    "                           n_features=50, \n",
    "                           n_informative=10, \n",
    "                           n_redundant=10, \n",
    "                           n_repeated=5, \n",
    "                           n_classes=2, \n",
    "                           n_clusters_per_class=2, \n",
    "                           weights=[0.5, 0.5],\n",
    "                           flip_y=0.01,\n",
    "                           class_sep=1.0,\n",
    "                           random_state=42)\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [f'feature_{i}' for i in range(50)]\n",
    "df = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "# Introduce some correlated features\n",
    "df['feature_corr_1'] = df['feature_0'] + df['feature_1'] + np.random.normal(0, 0.1, size=df.shape[0])\n",
    "df['feature_corr_2'] = df['feature_2'] - df['feature_3'] + np.random.normal(0, 0.1, size=df.shape[0])\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "df['target'] = y\n",
    "\n",
    "# Save the dataset as a CSV file\n",
    "df.to_csv('synthetic_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset created and saved as 'synthetic_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset with missing values in 'feature_1':\n",
      "   feature_1  feature_2   feature_3  feature_4\n",
      "0  54.967142  34.480348  103.182500 -64.506179\n",
      "1  48.617357  28.931847   95.789527 -52.919282\n",
      "2  56.476885  28.536595  105.029572 -58.544913\n",
      "3  65.230299  29.380465  127.380982 -55.791860\n",
      "4  47.658466  27.320350   76.380786 -44.875701\n",
      "\n",
      "Dataset created and saved as 'ml_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate a normally distributed feature\n",
    "feature_1 = np.random.normal(loc=50, scale=10, size=n_samples)\n",
    "\n",
    "# Generate 3 features that are correlated with feature_1\n",
    "feature_2 = 0.5 * feature_1 + np.random.normal(loc=0, scale=5, size=n_samples)  # Correlated positively\n",
    "feature_3 = 2.0 * feature_1 + np.random.normal(loc=0, scale=10, size=n_samples)  # More strongly correlated\n",
    "feature_4 = -1.0 * feature_1 + np.random.normal(loc=0, scale=5, size=n_samples)  # Negatively correlated\n",
    "\n",
    "# Introduce missing values in feature_1 (approximately 20% missing)\n",
    "missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.2), replace=False)\n",
    "feature_1[missing_indices] = np.nan\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature_1': feature_1,\n",
    "    'feature_2': feature_2,\n",
    "    'feature_3': feature_3,\n",
    "    'feature_4': feature_4\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First 5 rows of the dataset with missing values in 'feature_1':\")\n",
    "print(df.head())\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "df.to_csv('pre-ml_dataset.csv', index=False)\n",
    "\n",
    "print(\"\\nDataset created and saved as 'ml_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
